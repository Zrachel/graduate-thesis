\section{深度学习介绍}

\subsection{传统机器学习方法与局限性}

在之前的50年左右，传统的模式识别模型用手工定义的特征进行特征提取，通过对数据的分析选取可训分类器进行模型构建。 最近10年，借助现代计算机计算能力的提高和大数据量的爆发，神经网络方法得以重新广泛应用，在很多领域都达到非常好的效果， 我们称这种利用大规模网络进行模式识别方法为深度学习（Deep Learning）， 也叫End-To-End Learning。不同于传统模型采用固定特征，或者固定kernel（核函数）进行样本度量； 深度学习采用可训特征（或可训的kernel）， 然后将特征作为可训练的分类器输入， 进行训练， 如表\ref{Tab:dl_overview_compare}。

\begin{table}[ht]
\centering
  \begin{tabular}{|c||c|c|c|}
  \hline
   & 特征 & 分类器 & 特点\\
  \hline\hline
   传统方法 & 人工定义的特征 & 简单可训练分类器 & 特征设计费时，需强业务背景\\
  \hline
  深度学习 & 训练特征提取模型 & 复杂可训练分类器 & End-to-End learning，feature易操作\\
  \hline
  \end{tabular}
  \caption{深度学习与传统模式识别方法}
  \centering \label{Tab:dl_overview_compare}
\end{table}

历史上，第一个有学习功能的机器为1960年提出的Perceptron\cite{rosenblatt1960perceptron}，也是神经网络的一个基本单元。 Perceptron是一个简单特征提取器上加载的一个线性分类器： 


\begin{equation}
\label{Eq:Perceptron}
y=sign(\sum_{i=1}^N{w_iF_i(x)}+b)
\end{equation}


其中$x$为数据，$F_i(x)$为x的第i个特征，$w_i$为相应的特征权重参数， b为常参数， $sign$为分类器的非线性函数，对于二类分类，$sign$函数将结果映射到(0,1)。

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{Pictures/CNN/perceptron.jpg}\\
  \caption{Perceptront图例}\label{fig:perceptron}
\end{figure*}

目前最普及的实际应用也用到了线性分类器的一些变种，或者叫模板匹配（template matching）。 但是由于其底层的特征提取器需要反映特定信号的特点， 所以需要由特定领域专家来设置。 比如图像处理领域， 对于不同任务（ 图像分类， 图像分割， 图像跟踪等）， 所要求的特征就各不相同， 需要针对特定任务定义图像特征。 此外，传统方法也很难设计kernel, 从而不容易表达对距离的度量，仍然以图像来说，最简单的距离度量思路是对应像素相减，但是这显然不能表达图像语义层的相似信息。

为了能使特征更加灵活而且不过多依赖于专家指定的特征， 很多方法提出， 可以先人为定义简单特征， 然后通过无监督学习方法进一步得到中间层特征， 将其输入分类器进行最后分类。
典型的无监督特征学习方法如混合高斯模型\cite{jeong2004image,gray2001gauss,gauvain1992bayesian,reynolds2000speaker,reynolds1995robust, zivkovic2004improved,lee2005effective,yang1999gaussian}， K-Means\cite{liu2007computational,netzer2011reading,coates2011text,dy2004feature,coates2012learning}， 和Sparse Coding\cite{yang2009linear,boureau2008sparse,coates2011importance,coates2011analysis,gao2010local,mairal2010online}。 但这仍然不能解决以下几个问题：

\begin{enumerate}
	\item 建立传统模型代价大\\
	对每个领域，每个任务都需要设计特征。即便有了中间特征层进行无监督的特征选择， 庞大冗余的基础特征的设计也是耗时费力的。 随着工业界对不同任务的需求， 需要建立很多基础特征、 模型， 代价也很大。
	\item 无法很好地利用计算性能\\
	计算机性能的大幅提升本可以用来帮助加速机器学习， 但传统机器学习需要人工定义模型， 从而使模型规模受限，不能很好地利用计算性能和大量数据。 
	
	\item 人工定义特征效果欠佳\\
	目前， 自动学习的特征已经在图像、语音等很多领域强于人工定义的特征。 而且如果需要增加特征维度进行大规模学习就需要再手工定义更多特征， 而不能简单地够自动按比例扩大。
	
\end{enumerate}


\subsection{深度学习方法介绍}

深度学习是近几年来很热的机器学习算法， 在语音识别，计算机视觉，自然语言处理等领域打破了维持了多年的竞赛记录。以最典型的时序模型——语音识别的发展为例， 在1980年代早期， 主要应用Dynamic time Warping (DTW)\cite{juang1984hidden,myers1981comparative,rabiner1978considerations,berndt1994using}， 输入底层特征，通过无监督学习所得中间层特征输入分类器进行分类。 1985年后， \cite{huang1990hidden,rabiner1989tutorial,rabiner1986introduction}提出在中间层用隐马尔科夫模型描述一个序列出现的概率，得到进一步改进。 2010年左右， 使用深度神经网络进行逐层有监督学习达到最佳效果\cite{waibel1989modular,nakamura1989speaker}。


同传统方法的基本方法类似， 深度学习也是从数据分别生成底层特征， 中间层特征， 最后加入高层特征， 输入分类器进行分类， 即学习数据的结构化表示。 其形式化表示为：
\begin{equation}
y=f(W^kf(W^{k-1}f(...f(W^0X)...)))
\label{Eq:dl_formulation}
\end{equation}
其中， W为权重参数，k为层数，$W^iX$为输入X的线性表示， $f$为非线性函数, 如$tanh$ 函数。 深度学习就是优化$y$与groundtruth之间的距离，使之最小化。 从图像角度， 如图\ref{fig:feature_level}所示\cite{zeiler2014visualizing,zeiler2013stochastic}，图中(a)，（b）,(c)分别为自动学得的底层特征，中层特征和高层特征， 其中底层特征学习图像的浅层特征，在所有类别中共享， 如(a)中,学到的特征类似Gabor滤波器所提取的边缘特征\cite{ngiam2010tiled,shi1998gabor}， 从中间层到高层依次学习图像更深层的语义特征， 如有语义的显著图像区域， 高层特征更为稳定，也具有类属性。  从自然语言处理的角度， 初始输入为字符， 从底层向上依次学习单词， 短语， 长句， 文章。

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{Pictures/CNN/low_mid_high_feature.png}\\
  \caption{各层次feature}\label{fig:feature_level}
\end{figure*}


除了自动学习特征外， 深度学习和传统模型还有一个重要区别就是“深”。 一般来说， 深度结构由多层含参的非线性模型组成， 以形成特征的层次结构。
在\cite{bengio2007scaling,bengio2009learning}中讨论了深度模型的必要性。 浅层结构以现在的kernel machine\cite{scholkopf1999advances}为例， 如Support Vector Machine(SVMs)\cite{boser1992training,cortes1995support}。 这些方法定义特征
为一系列kernel函数连接而成的向量， 在训练数据中进行模板匹配， 如式\ref{Eq:kernel_fea_vec}所示。

\begin{equation}
	\label{Eq:kernel_fea_vec}
	\varphi(x) = [k(x,\mu_1),k(x,\mu_2),...,k(x,\mu_n)]
\end{equation}

式中， $\mu_i$为数据样本的一部分， 即模板样本； $\varphi(x)$为样本$x$的特征。 而后$\varphi(x)$通过线性组合进行分类:


\begin{equation}
	\label{Eq:kernel_predict}
	F(x) = W^T\varphi(x)
\end{equation}

可见， kernel方法就是一个简单的模板匹配层连接一个线性函数层， 由于方法中模板都是从原始训练数据中提取而得， 所以kernel machine的第一层
可视作一种简单的无监督方法， 只有式\ref{Eq:kernel_predict}中参数$W$的学习为有监督部分，没有涉及特征的层次结构， 因此不是深度模型。 同样，只有一个隐层的模型（如multilayer perceptron）也不算深度模型。 又如分类回归树（Classification and Regression Tree, CART）, 其中所有决策都是在输入空间定义的， 同样没有根据特征的层次结构进行学习， 因此也不属于深度模型。 虽然有一些理论保证浅层结构可以以任意精度拟合任何复杂函数\cite{hornik1989multilayer,hecht1989theory,komolgorov1956representation}， 它们却无法保证特征的高效表征。 而深度模型可以从根本上通过简单设置网络结构更有效地表示特定函数。 最典型的深度结构就是有多个隐层的神经网络， 其中每个隐层节点之间的连接权重（如图\ref{fig:perceptron}中的$w_i$）都是通过有监督学习而来的， 因此可以很好地表征数据属性。



\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.8]{Pictures/CNN/invariant-feature-crop.pdf}\\
  \caption{稳定特征生成方法}\label{fig:invariant_feature_learning}
\end{figure*}


神经网络在1940年代就开始了相关研究\citep{mcculloch1943logical,hebb1949organization}
， 但早期提出的神经网络多为线性回归的变种， 没有很大突破。 1965年， \cite{ivakhnenko1995review}提出了第一个深度学习模型——Group Method of Data Handling (GMDH)， 它通过回归方法进行网络训练， 再通过决策正则化去掉多余节点。 直到现在仍有很多应用以GMDH为基本框架， 如\cite{ikeda1976sequential,farlow1984self,madala1994inductive,kordik2003modified,witczak2006gmdh,
kondo2008multi}。 1960年代，受神经科学的启发， 神经网络也随之发展。 当时神经科学在猫的视觉皮层发现了简单细胞和复杂细胞\cite{hubel1962receptive}， 简单细胞对局部视觉特征（如边缘的方向）敏感， 会做出一定响应； 而复杂细胞相对简单细胞表现出更为鲁棒的响应， 即不受空间变化的影响， 可以在视网膜一块邻域区域中选出简单细胞的一个输入作为输出。 受此启发， 1979年 \cite{fukushima1979neural} 第一次在Neocognitron中引入了类似卷积神经网络的概念。 Neocognitron是为手写体识别提出的一个有层级结构的多层神经网络， 它不仅可以识别训练样本中的模式， 还可以识别训练样本经过平移， 旋转或其他变换后的模式。 其结构和现在我们用到的前向卷积神经网络比较相似， 层间连接也应用了现在我们常用卷积神经网络类似的部分权值共享（详见第\ref{sec:CNN}节）。 只是， Neocognitron通过无监督学习， 根据样本获得不同的pattern， 而不同于现在的有监督误差反传方法。 此外还有一些小差别，比如在pooling策略上， Neocognitron采用空间域平均， 而且虽然Neocognitron的层级较深， 但是它并没有在效果中考虑到深层带来的效果。 



由于90年代计算资源和数据量受限， 当隐层层数大于2的时候深度网络训练困难\cite{tesauro1992practical}， 所以之前神经网络的研究工作主要集中于浅层结构。 随着现在数据量的爆发， 神经网络的研究变得重新热门起来。 其基本思想是学习具有不变性的特征， 即，将输入数据映射到一个非线性的高维空间，使数据变得可分。 如\ref{fig:invariant_feature_learning}所示，输入经过非线性函数的映射到高维特征， 使数据变得可分， 但是这样的高维特征可能由于训练数据间差异导致不稳定，所以将高维数据中相似语义信息的成分通过池化（pooling）或累加等方法整合到一起， 形成稳定特征。





\subsection{深度神经网络结构与训练}

\subsubsection{网络结构}\label{sec:network_architecture}

从网络结构来分， 神经网络有前向网络（包括multilayer neural nets\cite{kuurkova1992kolmogorov,
hornik1989multilayer,hornik1991approximation}和 convolutional nets \cite{lecun1995convolutional,krizhevsky2012imagenet},）， 回馈网络(包括 stacked sparse coding\cite{yu2011learning}, deconvolutional nets\cite{zeiler2010deconvolutional})和双向网络(包括 deep boltzmann machines\cite{salakhutdinov2009deep,
srivastava2012multimodal,goodfellow2013multi}, stacked auto-encoder\cite{gehring2013extracting,vincent2010stacked,vincent2008extracting})三类\cite{lecun2013deep}。 目前， 前向网络使用最广， 如图\ref{fig:feed_forward}所示为一个前向网络的示意图。 

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{Pictures/CNN/feed-forward-crop.pdf}\\
  \caption{前向网络的网络结构}\label{fig:feed_forward}
\end{figure*}

其中， $x_i$表示第i层的输入， 整个网络的输入为$x_1$； $w_i$表示第i层的参数， $w_i^Tx_i$得到$x_i$的线性组合； $f$为非线性函数， 每一层通过非线性函数将输入数据映射到下一层， 如图中蓝色箭头所示。 经过每一层非线性映射$f_i$得到下一层输入，即
\begin{equation}
 x_i = f_{i-1}(w_{i-1}^Tx_{i-1})
\end{equation}

给定$x_1$对应的样本label $y$，我们的目标函数为$L(f_n(x_n),y)$， 其中$L$为损失函数。 常见的损失函数有平方损失， 指数损失， 对数损失等。 



\subsubsection{网络训练与误差反传}\label{sec:train_net}

自从1960年以来，不断有文章\cite{griewank2012documenta,director1969generalized,gray1965effect,bellman1962applied,
kelley1960cutting,bryson1961diffraction}从变分法中的欧拉-拉格朗日方程出发讨论如何利用梯度下降（gradient descent, 也叫steepest descent)的方法\cite{hadamard1908memoire} 来进行神经网络中多层非线性可导函数的优化。
在这样的神经网络中， 可以通过链式法迭代地则对每个神经元求导\cite{bellman1962applied}。 之后，1970年， 误差反传算法（back-propagation algorithm， BP）\cite{hecht1989theory,goh1995back} 首次在Linnainmaa的硕士毕业论文\cite{linnainmaa1970representation}中提出， 其中描述， 误差反传算发可以高效应用于任意神经网络结构。 此后BP迅速应用到神经网络的训练中， 并实现了给定可导函数自动求解导数和BP\cite{speelpenning1980compiling}。 其基本思想如下： 由图\ref{fig:feed_forward}和式\ref{Eq:dl_formulation}， 在前向算法中从输入到输出经过了一系列非线性可导函数$f$的映射。  每次针对一个样本， 计算出其输出和样本真实label之间的loss$L$， 然后自上而下逐层更新权重参数$w$（如图中向下的紫色箭头所示）， 使得当前误差可以减小。 用所有样本依次对权重做完更新后叫做一轮迭代。 不断做n轮迭代， 直到全局loss降到某个预设阈值， 这时网络就已经学好了。 其中， 对 $w_i$ 更新权重时采用梯度下降的方法：

\begin{equation}
\label{Eq:gradient}
	\Delta w_i = -\gamma \frac{\partial L}{\partial w_i}
\end{equation}

\begin{equation}
\label{Eq:gradient_descent}
	w_i = w_i + \Delta w_i
\end{equation}

式\ref{Eq:gradient}中， $\gamma$表示学习率， 就是每一步朝梯度下降方向走的步长， 是一个正的常数。 注意这里的负号表示loss是朝梯度下降的方向走。 式\ref{Eq:gradient_descent} 表示$w_i$ 每一步的更新。 但是$L$的表达式中难求对每个$w_i$的导数， 这里我们用$net_{j}$表示第j个隐层节点， 有

\begin{equation}
	net_j = \sum_i{w_{ij}o_i}
\end{equation}

即上一层输出到该节点的线性组合，其中$o_i$表示节点i的输出， $w_{ij}$表示节点i到j之间的连接， 有$o_{j} = f(net_j)$, 则$L$对$w_{ij}$的导数如式\ref{Eq:bq_basic}。


\begin{equation}\label{Eq:bq_basic}
\begin{split}
\frac{\partial L}{\partial w_{ij}} 
&= \frac{\partial L}{\partial net_j}\cdot \frac{\partial net_j}{\partial w_ij}\\
&= \frac{\partial L}{\partial net_j}\cdot \frac{\partial \sum_k{w_{kj}o_k}}{\partial w_ij}\\
&= \delta_{j}\cdot o_i
\end{split}
\end{equation}

其中$o_i$为前向过程中第i层的输出， $\delta_j = \frac{\partial L}{\partial net_j}$i为第i层中从后向前传递的错误， 我们称之为反向传播误差（backpropagated error）。 由公式\ref{Eq:gradient}，\ref{Eq:gradient_descent}和\ref{Eq:bq_basic}可知， i,j节点之间的权重的参数$w_{ij}$更新由节点j在输出端的反向传播误差乘以节点i的输出$o_i$（也就是节点j的输入端）而得， 如图\ref{fig:update_weight} 所示， 黑色线条表示前向通路， 红色线条表示反向传回的误差$\Delta w_{ij}$。

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.8]{Pictures/CNN/weight_update-crop.pdf}\\
  \caption{误差反向传播示意图}\label{fig:update_weight}
\end{figure*}


对于多层网络， 考虑如何计算反向传播误差$\delta_{i}$。 对于整个网络的输出单元$net_n$， 可以直接算得$\delta_n = \frac{\partial L}{\partial net_n}$， 例如对于$L$取square loss的情况， 有


\begin{equation}
\begin{split}
&	L(y,net_n)=\frac{1}{2}(net_n-y)^2 \\
&	\delta_n = \frac{\partial L}{\partial net_n} = net_n - y
\end{split}
\end{equation}


对于隐层节点$net_j$， 通过链式法则求导可得， 如图\ref{fig:update_weight_2}和公式\ref{Eq:hidden_update}所示。 
\begin{equation}\label{Eq:hidden_update}
\begin{split}
\delta_j
	&=\frac{\partial L}{\partial net_j}\\
	&=\sum_{k}\frac{\partial L}{\partial net_k}\cdot \frac{\partial net_k}{\partial net_j}\\
	&=\sum_k \delta_{k} \cdot \frac{\partial \sum_i{w_{ik}o_i}}{\partial net_j}\\
	&=\sum_k \delta_{k} \cdot \frac{\partial \sum_i{w_{ik}f(net_i)}}{\partial net_j}\\
	&=\sum_k \delta_{k} \cdot w_{jk} f'(net_j)
\end{split}
\end{equation}

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.8]{Pictures/CNN/weight_update_2-crop.pdf}\\
  \caption{$\frac{\partial L}{\partial net_j}$链式法则示意图}\label{fig:update_weight_2}
\end{figure*}

这样就可通过将误差逐层传递训练网络了。 整个误差反传算法流程如算法\ref{algo:BP_basic}所示。 在每一次迭代中， 首先前向计算出每个节点的输入$net_j$ 和输出$o_j$（见3~6行。 然后用误差反传算法， 在输出层直接用损失函数对节点求导计算反传误差（9-11行）， 其他隐层节点用公式\ref{Eq:hidden_update}计算反传误差（12-14行），最后更新权重参数（15-18行）。


\renewcommand{\algorithmicrequire}{ \textbf{Input:}}      %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}}     %UseOutput in the format of Algorithm
\begin{algorithm}
\caption{前向网络的误差反传算法}
\label{algo:BP_basic}
\begin{algorithmic}[1]  
\Require Data $x$, Label $y$
\Ensure  updated weight $w$
\For{t=1...T} \% 迭代次数
	\State \textbf{\textit{\% forward propagate}}
	\ForAll{unit $j$ not $\in$ input layer}
	\State $net_j \leftarrow \sum_i w_{ij}o_i$
	\State $o_j \leftarrow f(net_j)$
	\EndFor
	\State 
	\State \textbf{\textit{\% back propagate error}}
	\ForAll{ unit $k \in $ output layer }
		\State $\delta_k \leftarrow \frac{\partial L}{\partial net_k}$	\%直接求导计算	
	\EndFor	
	\ForAll{ unit $j \in $ hidden units }
		\State $\delta_j \leftarrow f'(net_j)\sum_k{w_{kj}\delta_k}$ \%根据公式\ref{Eq:hidden_update}
	\EndFor	
	\ForAll{ weight $w_{ij}$ } \%根据公式\ref{Eq:gradient_descent}和\ref{Eq:bq_basic}更新权重参数
		\State $\Delta w_{ij} \leftarrow \frac{\partial L}{\partial w_{ij}} = \delta_{j}\cdot o_i$
		\State $w_{ij} \leftarrow w_ij + \Delta w_{ij}$
	\EndFor
\EndFor
\end{algorithmic} 
\end{algorithm}


BP尽管在理论上支持深度网络的训练， 但是实际应用中BP一般只对较浅的模型有效\cite{schmidhuber2015deep}， 因此提出了很多BP的改进算法，如:

\begin{enumerate}
\item 关于BP加速\\
为了加速BP， Rumelhart等人在1986年引入动量（momentum）\cite{rumelhart1985learning}； Fahlman向线性化的激活函数$f$的斜率加入ad-hoc常数\cite{fahlman1988empirical}; Sperduti 
不但对x进行梯度下降，同时对梯度参数也进行梯度下降; \cite{sarkar1995methods} 将不同的bp加速策略做了比较。
\item 关于BP学习率\\
一般地， BP有两种学习率设定方法： 全局学习率 \cite{lapedes1988neural,vogl1988accelerating,lecun1993automatic} 和对每个权重都设一个自己的学习率\cite{jacobs1988increased,almeida1990speeding}。 在线学习中，$vario-\eta$\cite{neuneier1998train} 算法设置学习率与当前节点梯度的标准差成反比， 从而可以不受随机权重初始化的影响。

\item 如何增强模型普适能力（generalization）\\
为了提高模型的generalization，一些方法采用weight decay \cite{hanson1989comparing,weigend1991generalization}, 它本质上是一种正则化方法， 通过在loss function中加入权重参数的平方项来鼓励更小的权重， 惩罚大权重。 在线性模型中， 这种方法就是ridge regression。 加入weight decay一来可以限制模型的复杂度， 防止模型过拟合， 二来如果激活函数不能将输出规范化到输出应在的范围内， 大权重很可能让输出远远大于数据范围。 此外，大权重还会陷入Bias/Variance Dilemma\cite{geman1992neural}, 导致输出数据波动很大。 Hinton等人\cite{hinton1993keeping}从贝叶斯角度解释了weight decay可以从weight的高斯先验假设中得来， 证明了这不是一个简单的猜想。 另一些方法通过在weight中加其他先验实现同样的功能\cite{hastie1990generalized,hastie2009elements,mosteller1968data,mackay1992evidence}。

\end{enumerate}

	
总结一下， 误差反传算法通过求损失函数对每个权重参数的导数对它们进行更新， 更新过程从输出端到输入端依次向下传递误差。 与数值差分相比， 误差反传方法更适应计算机运算， 而且计算更为高效。 在实际应用中， BP也有一些条件， 当问题定义的复杂度很高， 不能写出解析解， 但是确定有解的时候我们采用BP进行网络优化。 否则可能用传统方法或者查找表方法会更为简单精确， 具体问题具体分析。 


在第\ref{sec:network_architecture}节中这三种为网络结构中， 卷积神经网络属于广泛应用的前向神经网络。 我们将在第\ref{sec:CNN}节中介绍卷积神经网络的结构与训练过程， 在第\ref{sec:cnn_configure}节中介绍我们的数据与网络配置， 最后在第\ref{sec:cnn_experiment}中给出实验结果。


\section{卷积神经网络}\label{sec:CNN}
在上一节中我们已经提到， 卷积神经网络（Convolutional Neural Networks， CNN）是前向网络的一种， 其原型始于1979年Fukushima提出的Neurocognitron\cite{fukushima1979neural}， 但在后期经过了很多改进。 1989年， Lecun将误差反传算法应用于带有共享权值（详见第\ref{sec:cnn_arch}节）的卷积神经网络进行训练\cite{lecun1989backpropagation}， 这一工作的内容是现在我们前向神经网络的重要部分。 在20世纪90年代， CNN在指纹识别\cite{baldi1993neural}、 人脸识别\cite{lawrence1997face}、 语音识别\cite{abdel2012applying}、 物体识别\cite{nebauer1998evaluation}、 文档识别\cite{lecun1998gradient} 上的应用相继出现出现并予以商用。  

近几年来， 随着计算机计算能力的增强和数据量的激增， 卷积神经网络在各个领域的应用更为广泛， 也得到了更好的效果。 以机器视觉领域为例， 09年， Turaga（MIT）等人用学习的方法， 以卷积神经网络为模型训练生成相似图\cite{turaga2010convolutional}， 使得和传统人工定义分割函数的方法相比，分割精度有了很大提升。 2011年， Sermanet（NYU）和Lecun将CNN应用于交通标志分类\cite{sermanet2011traffic}， 在GTSRB竞赛数据集\cite{stallkamp2011german}上达到99.17\%的最高准确率， 超过人的识别能力。 对于图像分割任务， 很多算法需要首先生成一张相似图（affinity graph）然后在其上做分割。 2012年， Alex Krizhevsky（Google）在ImageNet\cite{deng2009imagenet}上应用深度卷积神经网络进行图像分类\cite{krizhevsky2012imagenet}。 该任务要将130万高清图像分成1000类， Krizhevsky采用5层卷积层， 共50万个神经元节点，6000万个参数进行训练，达到了当时图像分类的最佳效果。 2014年， Taigman（Facebook）等人应用卷积神经网络建立了DeepFace\cite{taigman2014deepface}， 在包含4000多人的4百万人脸图像数据集上进行训练， 在LFW数据集\cite{huang2007labeled}上进行人脸识别， 结果达到97.35\%的识别率， 近乎和人的识别水平相当。 整个过程为检测人脸->对齐->3d人脸表征->分类， 其中分类用的是多达1.2亿个参数的9层非标准卷积神经网络。 同年， Simonyan（Oxford）等人将深度卷积神经网络应用到视频进行行为识别\cite{simonyan2014two}。 他们提出了一个双CNN框架， 分别在时间和空间方面各应用一个CNN， 并证明再多帧的深度光流上应用ConvNet可以在有限数据集上达到很好的效果。 Toshev和Szegedy（Google）用深度神经网络进行人体姿态估计\cite{toshev2014deeppose}。 该方法在CNN上实现一个回归问题， 回归目标为人体节点（如关节处）， 然后将多个这样的深度神经网络级联， 在姿态估计方面达到了目前最佳的准确率。 


此外， DeepMind团队（Google）\cite{mnih2013playing}在卷积神经网络上进行增强学习，实现了机器自动打游戏。 Ciresan\cite{cirecsan2013mitosis}将深度神经网络应用于医学图像， 进行癌细胞检测。  自然语言处理方面， Denil(Oxford)等人通过将文档嵌入一个低维向量空间来提取文档语义信息并做归类，  该模型基于卷积神经网络， 在语义级别和文档级别同时学习卷积核。在框架方面， 2014年， Goodfellow在神经网络中提出了一个估计生成模型的框架\cite{goodfellow2014generative}， 通过同时学习两个相互对立的模型， 一个生成模型G拟合数据分布，一个判别模型D估计样本从训练数据得到（而不是从G得到）的概率。最终目标最大化D犯错误的概率（即让G尽可能地模拟原始数据分布）， 其中G和D都是多层perceptron。 神经科学方面， Cadieu（MIT）将卷积神经网络应用下颞叶皮层信号的建模\cite{cadieu2014deep}， 揭示了下颞叶皮层在基于视觉的物体识别任务上对信息的表征能力。 也有工作将卷积神经网络应用于功能性磁共振成像（fMRI）数据大脑神经信号解码\cite{firat2014learning}， 结果和其他模式分析技术相比有所提高。 Barnickel等人基于卷积神经网络提出了SENNA， 一个快速精确地从生物类文献中提取分析予以相关性的神经网络\cite{barnickel2009large}。 

在第\ref{sec:introduction_p300}节中我们已经介绍过P300神经信号的产生及其对应的P300检测任务。 受以上文章启发， 本文中采用卷积神经网络为P300信号建模， 并应用于P300信号检测。 下面， 我们先来看一下卷积神经网络是如何工作的。


\subsection{CNN网络结构}\label{sec:cnn_arch}

\begin{enumerate}

\item 卷积层\\
如图\ref{fig:feed_forward}所示， 卷积神经网络也是一个层级结构的前向神经网络。 除了输入层和输出层， 中间的隐层的目标是从原始数据学得一系列非线性组合， 使得可以很好地结合当前任务表征信号。 以图像为例， 输入为像素级数据， 每个像素本身不含有太多信息， 但是组合起来所得到的更高级别的特征可以表示物体类别等信息。 尽管我们的神经信号不同与图像信号， 但是文本分析\cite{collobert2008unified}， 语音识别等其他任务都受了图像的启发， 因其方便可视化， 所以我们采用图像进行解释。


\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.8]{Pictures/CNN/single_layer-crop.pdf}\\
  \caption{局部连接示意图}\label{fig:local_connectivity}
\end{figure*}


以输入层和第一层隐层中的一个神经元节点连接为例， 如图\ref{fig:local_connectivity}所示。 图中黄色区域表示输入， 为32*32*3大小的数据， 比如可表示一张深度为3（3通道）， 宽、高均为32的图像。 图中的蓝色圆点表示与输入层相连的一个隐层节点， 连接项由权重参数组成。 和大脑中视觉皮层的接收域（receptive fields）一样， 我们在卷积神经网络的隐层中也设置接收域， 该接收域可以视作一个在输入中进行扫描的线性滤波器， 为了减少参数数量， 该滤波器中的权重参数在输入层中共享。 直观地想， 共享也是有原因的。 因为该接收域的目的是发现输入空间中的一些特定模式， 这是不需要受到检测位置在图像中位置的限制的， 这也解释了为什么CNN网络学出的特征具有平移不变性。 换句话说， 其目的是使我们要找的特定模式与其在输入数据的位置无关。 卷积神经网络中， 我们称该接收域为核（kernel）。 例如图中为5*5的kernel， 深度为3（和通道数相同）。 由于CNN的连接层中共享权值， 所以该节点与输入层相连接的参数个数总共只有5*5*3个。 在输入层中， 从原点（左上角）开始分别向右方和下方扫描， 假如每隔1个像素进行扫描， 每个5*5*3的区域块经5*5*3的kernel卷积得到一个标量， 那么一个数据32*32*3的数据经过卷积也就可以得到一个（32-5+1）*（32-5+1）大小的数据。

对于隐层中有多个节点的情况， 我们在隐层的深度维进行扩展。 假如我们希望隐层有N个节点， 就令隐层的深度为N， 那么隐层大小即为（32-5+1）*（32-5+1）*N。 刚才我们对于输入层每隔一个像素进行一次卷积， 但是为了减小计算复杂度， 我们还可以设置卷积的步长， 也就是每隔多少个元素进行一次卷积操作， 我们称这个步长为stride， 如图\ref{fig:stride}所示为输入大小为7*7的图像， stride为2的情况， 可见此时对应的输出长和宽为(7-3)/2+1。 即给定输入数据大小$(D_h,D_w,D_c)$， kernel大小$(k_h, k_w)$, stride 大小s， 要求卷积神经网络的隐层节点为K， 隐层大小为$((D_h-k_h)/s+1,(D_w-k_w)/s+1,K)$， 有$k_h\times k_w\times D_c\times K$个参数。


\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.8]{Pictures/CNN/stride-crop.pdf}\\
  \caption{stride示意图}\label{fig:stride}
\end{figure*}




\item 降采样层\\
1998年， Yan Lecun等人提出， 在网络构建时可以在CNN网络的卷积层后面跟一个降采样层， 用来对卷积结果中的每个小窗口做平均或者最大化pooling。 有两个原因， 一来这样可以减少下一层的数据数据规模， 降低计算量； 二来这样在一个小区域的节点中做平均或最大化选择可以增加模型的适应性， 减少网络受旋转、 平移等变换的影响。 将卷积层和降采样层结合实际是受Hubel和Wiesel在生物实验上结果的影响\cite{hubel1962receptive}。 和卷积层类似， 降采样层也有相应的窗口高、宽和stride。 经过降采样层， 使得一块区域的数据被压缩， 分在不同的“池子”里， 因此降采样层所做的事情也叫做池化。 除了均值和最大值， 还可以取$L_1$, $L_2$范数进行降采样。



\item 全连接与输出层\\
一个典型的卷积神经网络如图\ref{fig:lenet}所示。 图为Yan Lecun的为MNIST数据集设计的卷积神经网络LeNet5\cite{lecun1998gradient}。 应用在著名的手写MNIST数据集\cite{lecun1998mnist} 有60000个训练数据和10000个测试数据， 样本大小为32*32， 均为数字0到9， 共10类。 从图中可见， 网络输入为数字图像， 前两层为有6个高、宽均为5的kernel（深度为6）的卷积层和窗口高、宽均为2的降采样层。 3,4层类似是一个卷积加降采样层。 此外， 在卷积神经网络中还有两层全连接层， 由全连接层连接的神经元两两连接， 也就是说， 和全连接层相连接的层间权重个数等于两层神经元个数的乘积， 用来增强网络的表达能力。 最后是输出层， 对于10类分类问题， 可以设置输出层为10个节点， 最后采用Softmax(如式\ref{Eq:softmax})或Sigmoid(如式\ref{Eq:sigmoid})函数进行分类。 在LeNet中， 最后一层采用的RBF网络， 达到测试误差达到0.95\%。 训练过程可参照LeNet5的一个demo\footnote{http://yann.lecun.com/exdb/lenet/}。

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.7]{Pictures/CNN/lenet.jpg}\\
  \caption{LeNet5， 一个为手写数字做分类的卷积神经网络。\cite{lecun1998gradient}}\label{fig:lenet}
\end{figure*}


\begin{equation}
S(t)=\frac{1}{1+e^{-t}}\label{Eq:sigmoid}
\end{equation}

\begin{equation}
S(t) = \frac{e^{t}}{\sum_{l=1}^k{e^t}}\label{Eq:softmax}
\end{equation}

其中， $t$表示输入$x$的线性组合， 即$t=w^Tx$。




\item 规则化层\\
通常，我们希望输入数据是经过规则化（normalization）的， 即对数据进行白化（whiten）， 以减少数据variance产生的波动， 通过减均值，除以方差完成。 该层通常加在数据层之前。

\item 激励层\\
如果卷积神经网络中， 卷积层经过线性组合之后不加非线性函数的话， 那么整个网络就完全可以有一个线性函数来表示了。 为了增强模型的表达能力， 我们加入非线性层， 使所得结果 稀疏化， 或者得到侧抑制， 我们也称非线性层为激励层， 相应的非线性函数称为激励函数。 通常， 我们所用的非线性函数有rectified linear units（ReLU, 如式\ref{Eq:relu}）， Component-wise shrinkage, tanh（如式\ref{Eq:tanh}）, sigmoid（如式\ref{Eq:sigmoid}）和winner-takes-all。

\begin{equation}\label{Eq:relu}
 relu(x) = max(x,0)
\end{equation}

\begin{equation}\label{Eq:tanh}
 tanh(x) = \frac{1-e^{-2x}}{1+e^{-2x}}
\end{equation}

\begin{equation}\label{Eq:sigmoid}
 sigmoid(w,x) = \frac{1}{1+e^{w^Tx}}
\end{equation}
\end{enumerate}

综上， 现在用的卷积神经网络每个卷积层经过数据规则化， 卷积， 非线性激励， 池化（降采样） 四个模块。 整个卷积神经网络如图\ref{fig:whole_cnn_form}所示。

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.7]{Pictures/CNN/cnn_net_diag-crop.pdf}\\
  \caption{卷积神经网络结构示意图}\label{fig:whole_cnn_form}
\end{figure*}







\subsection{CNN网络训练}

CNN模型通常用随机梯度下降\cite{nemirovski1978cezari,speelpenning1980compiling}算法进行优化。 随机梯度下降算法源于随机优化。 随机优化在统计学习理论中非常常见， 即求参数$\theta$来最小化loss的期望。


\begin{equation}
\label{Eq:stocastic_optimization}
	\theta^* = arg min_{\theta} E(L(f(\theta,x),y))
\end{equation}


在线学习中有一个简单算法求这样的$\theta$， 叫做在线梯度下降\cite{zinkevich2003online}： 在每一步迭代中， 按式\ref{Eq:online_gd}更新权重, 其中$\theta_k$为第k步的$\theta$值， $\eta$为学习率。


\begin{equation}
\label{Eq:online_gd}
	\theta_{k+1} = \theta_k - \eta g_k
\end{equation}

也就是更新方向为第k步时梯度下降最快的方向。 这种把数据看做一个源源不断的流数据， 每一轮（epoch）迭代对60000个训练样本分别进行权重更新的方法叫做随机梯度下降（Stochastic Gradient Descent， SGD)。 2006年， $Mike O'Neill$ 在MNIST上训练CNN， 采用4个隐层， 共3215个隐层神经元节点， 134066个权重参数， 184974条网络连接（网络连接数大于权重参数， 因为网络中有两个卷积层， 其中是共享权值的）。 一轮迭代大约用40分钟， 由于收敛大约需要30轮迭代， 所以整个训练时间大约需要20个小时。 用该方法， 他在MNIST上将测试集误差降到1.4\%。





\section{ConvP300Net进行P300检测}\label{sec:cnn_configure}
在第一章中， 我们已经介绍了oddball实验以及P300波形检测任务。 在这一章中， 我们进行第一章中提到的P300波形检测中的第一个任务——p300波形二类分类， 即将数据段分为是P300和不是P300。 之所以只做了P300分类而没有做字符识别是因为数据量有限， 如果做36类分类的话数据量不够。 下面我们来介绍数据集。


\subsection{数据集}
我们所用的数据来自BCI竞赛的第二个数据集\cite{blankertz2006bci}。该数据集由纽约健康部门Wadworth中心提供。 该数据为从两个测试个体上采集的EEG信号， 其中有受刺激所产生的的P300波形。 生成该数据时， 指定让实验主体看一个预先设定的字符， 然后如图\ref{Fig:p300matrix} 所示的矩阵以5.7Hz的频率进行行列交替随即闪烁。 当先后交替闪烁的一行一列相交于指定字符时产生一个P300信号。 

实验时， 在最开始先空（没有任何闪烁）2.5s做准备。 然后每个行/列随即闪烁100ms， 每次闪烁后， 矩阵再空75ms。 接着， 每行/列再随机闪烁， 对每个指定字符所在的行/列各闪烁15次。 因此， 对每个字符， 理论上都有30个P300波形。 信号从64个通道， 以240Hz的频率进行采集， 然后将信号通过0.160Hz的带通滤波器\cite{sharbrough1991american}。 对每个实验主体， 训练数据和测试数据分别指定85个和100个字符， 那么训练集的P300波形共有85*2*15个。 根据我们上面的数据处理方法， 得到训练集和测试集上的数据段个数如表\ref{tab:p300_experiment_conf}所示。 


\begin{table}[ht]
\centering
  \begin{tabular}{|c||c|c|}
  \hline
   & 训练数据 & 测试数据\\
  \hline\hline
   P300 & 2550 & 3000\\
  \hline
  非P300 & 12750 & 15000\\
  \hline
  \end{tabular}
  \caption{各类数据数目}
  \centering \label{tab:p300_experiment_conf}
\end{table}


\subsection{相关工作}
在该数据集上， 已有的一些工作主要用多分类起来提高分类精度， 并用不同方法进行数据清洗。
\begin{enumerate}
	\item 基于SVM\cite{rakotomamonjy2008bci,blankertz2002classifying}\\
	在基于SVM的方法中，信号首先经过0.5-8Hz的带通滤波器， 然后通过独立成分分析（ICA）去除眼动噪音数据， 最后通过Bagging， 将多个SVM分类器所得结果进行投票。
	\item 基于成分分析（ICA 和 PCA）\\
	Zhou Zongtan等人提出， 数据可以首先通过主成分分析进行预处理， 用t-统计值在每个通道上进行特征选择。
	\item 基于LDA\cite{breiman1996bagging}\\
	Lin Zhonglin 提出可以用LDA作为分类器， 提高分类器的判别性， 并通过bagging将多个LDA分类器所得结果进行投票。
	\item 基于神经网络\cite{felzer2003analyzing,anderson1995determining,cecotti2008time,masic1995neural,masic1993neural}\\
	基于神经网络的方法中， 通过对P300波形建立卷积神经网络模型来对P300波形进行分类。
\end{enumerate}


\subsection{ConvP300Net模型}

由于已有相关工作在p300检测上有所成效， 所以在我们的方法中， 研究采用卷积神经网络进行P300的检测。 在数据清洗与数据预处理方面， 我们采用和Hubert Cecotti等人在用卷积神经网络时对数据处理相同的方法\cite{cecotti2011convolutional}。 首先对数据进行降采样， 采用120Hz的采样率（而不是240Hz）进行数据采样。 然后通过0.1Hz到20Hz的带通滤波器， 在第二章中我们已经讲过EEG所采集的信号主要是低频信号， 因此这样有助于减少五官噪声。 过滤后的数据通过减均值除以方差来进行规则化（normalization）。 最后，我们将数据进行分段， 并标号（计算label）。 定义每个数据段长为$T_s$， 采样率为$f$, 则每个数据段持续时间长$T_s*f$。 这里我们取$T_s$为650ms， 即每一次从上一次闪烁和空闲之后开始采样， 采样650ms， 共$0.65s*120Hz=78$个采样点长的数据， 数据段之间有覆盖， 如图\ref{fig:data_segmentation}所示， 红色的箭头表示每一个行/列闪烁的起始时刻， 黑色的箭头表示数据分段的段起始点， 每段持续650ms， 可见数据段之间有覆盖， 每段标号以该段起始的那100ms闪烁为标准， 如果这100ms内闪烁的行/列包括指定字符， 其标号就为1， 否则指定标号为0。 整个数据预处理过程的流程图展现在图\ref{fig:data_preprocess_flow}中。

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{Pictures/CNN/data_segmentation-crop.pdf}
  \caption{P300检测——数据段分割}\label{fig:data_segmentation}
\end{figure*}

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.8]{Pictures/CNN/cnn_flow-crop.pdf}
  \caption{P300检测——数据预处理}\label{fig:data_preprocess_flow}
\end{figure*}



我们的网络结构如图\ref{fig:p300cnn_network}所示， 我们称之为PConv网络。 由6层网络($L_0$到$L_5$)组成：


\begin{enumerate}
	\item $L_0$ \\
		数据层由上面数据预处理过程得到的数据构成， 每个数据有64个通道， 78个采样点组成。 也就是在空间和时间维度分别为64维和78维。 
	\item $L_1$ 局部连接层\\
		$L_1$局部连接层类似卷积层， 只不过卷积层kernel中的权重参数在空间扫描的时候是共享的， 而局部连接层是不共享的。 对于大小为$(k_h, k_w)$的kernel， 一共可以卷积$(D_h-k_h+1)*(D_w-k_w+1)$次， 对于有$K$个节点的全连接层， 其参数数目为$k_h*k_w*K$； 对于局部连接层， 其参数数目为$(D_h-k_h+1)*(D_w-k_w+1)*k_h*k_w*K$个。 在Cecotti的工作中\cite{cecotti2011convolutional}， 第一层数据输入后采用卷积层将样本进行操作， 卷积核大小为$64*1$， 即在空间域对数据进行卷积， 在时间维度共享这个卷积核的权重。 但是， 时间维度为我们人为划分的数据段， 并没有道理说， 所有时间点上64个通道起相同作用， 即不应该让不同时间采样点共享权重。 所以在我们的工作中， 依然采用$64*1$的核函数， 但是在不同时间点有不同的权重大小， 即第一层采用局部连接层对空间域进行卷积。	
		
	\item $L_2$ 卷积层\\
		在$L_2$， 我们采用卷积层将$L_1$的输出进行卷积。 对于大小为$N_s*1*78$的输入， 卷积核大小为$1*1$,  也就是在空间域分别对每一个神经元进行卷积， 设置$5N_s$个卷积核， 其中$N_s$为$L_1$的节点数（深度）。 然后， 我们在L2后面接一层降采样层和激励层（考虑到画图简洁性并未在图中画出）。 降采样层分别在高、宽进行长为13，1的降采样， 降采样stride为13，也就是进行无重合的降采样， 并将得到的结果输入激励层进行非线性映射， 这里我们采用非线性函数$Relu$。
	
	\item $L_3$ 全连接层\\
	全连接层设置100个节点， 与$L_2$全连接。经过降采样，$L_2$输出共有$5N_s*6$个节点， 设置全连接层有100个节点， 那么$L_3$中共有$5N_S*6*100$个连接。

	\item $L_4$ 全连接层\\
	和$L_3$层类似， $L_4$也设置为全连接层，由于是二类分类， 我们设置$L_4$为预测输出， 即只有两个节点， 一个表示正样本（有P300波形）， 一个表示负样本（没有P300波形）。 在预测时， 我们取前向网络计算所得值大的作为类别输出。 在训练时， 我们将其输入$L_5$ loss层。

	\item $L_5$ Loss层\\
	在训练时， 最后我们将预测样本类别概率与真实样本标号输入loss层求loss以便通过误差反传算法训练网络。

\end{enumerate}


\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.7]{Pictures/CNN/cnn_network-crop.pdf}
  \caption{ConvP300Net}\label{fig:p300cnn_network}
\end{figure*}






\section{实验结果及实现}\label{sec:cnn_experiment}
\subsection{PConv的学习}\label{sec:weighted_loss}
整个网络的学习采用误差反传算法， 从后往前对每一层分别求导将误差传回， 如第\ref{sec:train_net}节所示。 但有两点不同， 一是在PConv中， 我们采用不同的loss函数， 二是选用Relu非线性映射， 但是该函数在零点不可导， 下面我们来分别说明。 
\begin{enumerate}
	\item Loss 层的设计\\
		由表\ref{tab:p300_experiment_conf}可知， 该样本存在正负样本不均衡的问题， 正样本与负样本比例为1：5。 因此我们考虑样本均衡方法， 如Hubert的工作中分别设置了5个分类器， 每个分类器用全部正样本和$\frac{1}{5}$的负样本做训练。 但是， 这样还需要考虑到后续分类器做融合时ensemble learning带来的提升效果。 为了在同一个网络中进行训练并作性能测试， 我们更改loss层定义， 改用weighted loss。 
		在weighted loss中， 我们受boosting启发\cite{}， 赋给正样本较大权重， 赋给负样本较小权重。 这样在调整的时候就可以更注重正样本的调整。 采用euclidean loss, 其loss定义如下：

\begin{equation}
	L(f(x),y) = \sum_{i=1}^N {g(y_i)(f(x_i) - y_i)^2}
\end{equation}
其中$g(y)$为根据$y$设置的权重：
\begin{equation*}
  g(y)=\left\{
    \begin{array}{lr}
        \frac{1}{6}, \,\, if\,\, y=1\\
        \frac{5}{6}, \,\, if\,\, y=-1\\
    \end{array}
  \right.
\end{equation*}
所以， 
\begin{equation}
	L(net_n,y) = \frac{1}{2}[\frac{5}{6}(y+1) + \frac{1}{6}(1-y)](net_n-y)^2
\end{equation}
	而后其更新可以根据算法\ref{algo:BP_basic}得到, 第n层的反向传播误差为：
	
\begin{equation}
	\delta_n = \frac{\partial L}{\partial net_n} = [\frac{5}{6}(y+1) + \frac{1}{6}(1-y)](net_n-y)
\end{equation}
	而对之前的隐层节点$net_j$, 通过链式求导法则可得，
\begin{equation}
	\delta_j = \sum_k{\delta_k\cdot w_{jk}f'(net_j)}
\end{equation}	
	其中$k$是所有与j节点相连的下一层节点编号。
	
	

	\item 非线性映射 Relu\\
	神经网络的非线性激活函数有多种选择， sigmoid, tanh 和 ReLU的表达式我们已经在上文中介绍过， 其图形如图\ref{Fig:activation}所示。 很多工作中显示ReLU有助于提升卷积神经网络的效果\cite{glorot2011deep,maas2013rectifier}。 其区别主要是sigmoid函数的值域在[0,1]而ReLU的值域在0到正无穷。 所以sigmoid可以用来为概率建模， 而ReLU可以为正实数建模。 然而， 从图\ref{Fig:activation}可知ReLU在0处不可导， 所以ReLU采用分段求导（x<0时g'(x)=0,x>0是g'(x)=1）。
	
	\begin{figure*}[htb]
  	\centering
  	% Requires \usepackage{graphicx}
  	\includegraphics[scale=0.8]{Pictures/CNN/activation.jpg}
  	\caption{激励函数sigmoid,tanh,ReLu,softplus}\label{Fig:activation}
	\end{figure*}
	
	相比其他非线性映射函数， ReLU有一些显著优势。 首先， 它具有sparse约束性， 因为从概率角度， 只有50\%的神经元被激活。 其次， ReLU具有没有tanh， sigmoid的梯度vanishing和exploding问题\cite{zeiler2013rectified}。 最后， ReLU具有计算简单的性质， 所以更为高效。
	
\end{enumerate}



\subsection{实验结果}
在实验中， 我们将我们的方法与Hubert Cecotti的结果做以比较， 用了以下评价标准：
\begin{enumerate}
	\item TP,
	true positive 的缩写， 表示分正确的正样本数。

	\item TN,
	true negative的缩写， 表示分正确的负样本数。

	\item FP,
	false positive的缩写， 表示错分成正样本（实际是负样本）的数目。

	\item FN,
	false negative的缩写， 表示错分成负样本（实际是正样本）的数目。

	\item Reco.,
	recognition rate表示识别率， 表示所有样本中分对的比例。

	\item Recall,
	召回率表示分对的正样本数占所有分对样本数的比例， 即
	\begin{equation}
		Recall = \frac{TP}{TP+TN}
	\end{equation}

	\item Precision,
	精度表示分对的正样本数占所有估计成正样本的样本中的比例， 即
	\begin{equation}
		Precision = \frac{TP}{TP+FP}
	\end{equation}

	\item F-value,
	作为一个综合评价指标， F-value将Recall 和 Precision都考虑了进来：
	\begin{equation}
		F-value = 2\frac{Recall \cdot Precision}{Recall+Precision}
	\end{equation}
\end{enumerate}


采用各网络时， A, B两个人的结果分别如表\ref{tab:p300_cnn_A}， \ref{tab:p300_cnn_B}所示。 其中， CNN1, CNN3, MCNN1, MCNN3 \cite{cecotti2011convolutional}的基本模型一样， 且也是采用卷积神经网络对数据进行建模。 不同之处是， $L_1$采用卷积神经网络， 在数据78个时间点的每个时间点共享空间域64个通道的权值； $L_2$采用先降采样后卷积（而非我们方法中的先卷积后降采样）的策略以降低模型复杂度，且采用tanh作为激励函数（而我们的PCNN, PsCNN, PwCNN, PConv方法均选用Relu）。 我们将该模型记为CNN1。
在CNN1的基础上， CNN3 在$L_1$中只设置一个节点， 也就是说$L_1$的深度为1。 此外， 如表\ref{tab:p300_experiment_conf}所示， 该样本存在正负样本不均衡的问题， 正样本与负样本比例为1：5。 为解决该问题， MCNN中提出用多个分类器。 在MCNN1中，将所有负样本分为5份， 用5个分类器分别训练全部正样本加$\frac{1}{5}$的负样本。 在MCNN3中，为了避免网络随机参数初始化带来的波动问题选用了3个分类器中最好的结果作为最后的分类结果。 与CNN1相比， 我们的PCNN方法将$L_2$中的降采样与卷积交换顺序， 即先卷积， 后降采样， 这是因为参考其他网络（如LeNet， AlexNet）都是先做卷积再做pooling， 而且这样可以提高模型参数规模， 更好地拟合数据； PsCNN在PCNN的基础上在$L_4$输出概率后加了一层非线性映射， 希望提高模型适应性； PwCNN是PCNN加了weighted loss版本，详见第\ref{sec:weighted_loss}节； 在PwCNN的基础上， PConv模型将第一层从卷积层改为局部连接层， 使得64维通道的权重在不同时间点不共享。 



\begin{table}[ht]
\centering
  \begin{tabular}{|c||c|c|c|c|c|c|c|c|}
  \hline
   & TP & TN & FP & FN & Reco.(\%) & Recall & Precision & F-value \\
  \hline\hline
	CNN1 & 2021 & 10645 & 4355 & 979 & 70.367  & 0.674  & 0.317  & 0.431 \\
	\hline
	CNN3 & 1827 & 10554 & 4446 & 1173 & 68.783  & 0.609  & 0.291  & 0.394 \\
	\hline
	MCNN1 & 2071 & 10348 & 4652 & 929 & 68.994  & 0.690  & 0.308  & 0.426 \\
	\hline
	MCNN3 & 2023 & 10645 & 4355 & 977 & 70.378  & 0.674  & 0.317  & 0.431 \\
	\hline
	PCNN & 2079 & 10597 & 4403 & 921 & 70.422  & 0.693  & 0.321  & 0.439 \\
	\hline
	PsCNN & 2013 & 10483 & 4517 & 987 & 69.422  & 0.671  & 0.308  & 0.422 \\
	\hline
	PwCNN & 2194 & 11071 & 3929 & 806 & 73.694  & 0.731  & 0.358  & 0.481 \\
	\hline
	PConv & 2216 & 11402 & 3598 & 784 & 75.656  & 0.739  & 0.381  & 0.503 \\
  \hline
  \end{tabular}
  \caption{实验人A —— P300 检测各卷积神经网络结果}
  \centering \label{tab:p300_cnn_A}
\end{table}


\begin{table}[ht]
\centering
  \begin{tabular}{|c||c|c|c|c|c|c|c|c|}
  \hline
   & TP & TN & FP & FN & Reco.(\%) & Recall & Precision & F-value \\
  \hline\hline
   CNN1 & 2035 & 12039 & 2961 & 965 & 78.189 &  0.678 & 0.407 & 0.509\\
  \hline
   CNN3 & 2006 & 11348 & 3652 & 994 & 74.189 & 0.669 & 0.355 & 0.463\\
  \hline
   MCNN1 & 2202 &11453 & 3547 & 798 & 75.861 & 0.734 & 0.383 & 0.503 \\
 \hline
  MCNN3 & 2077 & 11997 & 3003 & 923 & 78.189 & 0.692 & 0.409 & 0.514 \\
\hline
  PCNN & 2145 & 12114 & 2886 & 855 & 79.217  & 0.715  & 0.426 & 0.534 \\
\hline
  PsCNN & 2194 &12383 & 2617 & 806 & 80.983 & 0.731 & 0.456 & 0.562 \\
\hline
  PwCNN & 2276 & 12745 & 2255 & 724 & 83.450 & 0.759 & 0.502 & 0.604 \\
\hline 
  PConv & 2537 & 12679 & 2321 & 463 & 84.533 & 0.846 & 0.522 & 0.646 \\
  \hline
  \end{tabular}
  \caption{实验人B —— P300 检测各卷积神经网络结果}
  \centering \label{tab:p300_cnn_B}
\end{table}



我们将PConv, PwCNN, PsCNN的训练loss随迭代次数下降的曲线画出来， 如图\ref{fig:train_loss}所示。 可见， PConv和PwCNN的训练loss下降曲线类似， 而加了sigmoid非线性映射后， PsCNN很快下降后几乎损失函数不降了。 这是因为加了sigmoid后原本概率被映射到[0,1]，使得误差非零即一。  大约5次迭代后达到收敛。


\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{Pictures/CNN/train_loss.png}
  \caption{ConvP300Net 训练loss-迭代次数}\label{fig:train_loss}
\end{figure*}


将PConv, PwCNN, PsCNN的测试loss随迭代次数变化的曲线画在图\ref{fig:test_loss}。 如图可见， 在测试集上的loss总体上呈下降趋势， 但可能存在局部波动。 最后， 我们将三种方法的测试准确率展现在图\ref{fig:test_acc}中。 可见， 三种方法的测试准确率都较为稳定， PConv的准确率最佳。


\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{Pictures/CNN/test_loss.png}
  \caption{ConvP300Net 测试loss-迭代次数}\label{fig:test_loss}
\end{figure*}

\begin{figure*}[htb]
  \centering
  % Requires \usepackage{graphicx}
  \includegraphics[scale=0.9]{Pictures/CNN/test_accuracy.png}
  \caption{ConvP300Net 测试准确率-迭代次数}\label{fig:test_acc}
\end{figure*}




\subsection{实现方法}
我们的实现基于Berkeley Vision and Learning Center 的Yangqing Jia开发实现的Caffe\cite{jia2014caffe}， 为开源代码。 它可以实现在CPU和GPU上运行， 这里我们选用的GPU为NVIDIA出的GT Force 580， 基于cuda进行并行加速。 Caffe用纯C++实现，提供了Python，Matlab脚本和命令行接口。 通过配置文件可以定义网络结构和参数配置。 实验中， 我们将数据经过以下处理， 先将所有数据分为多个batches。 如15300个训练数据分为100个batch，其中95\%作为训练数据， 5\%作为测试数据， 每个batch有153个数据。 然后经过leveldb将数据转换成kv存储， 然后在网络中进行训练。 数据在整个网络中都通过块（blob）的形式定义。 在caffe的基础上， 本文在数据层进行数据生成与转换， 在layer中增加了weighted euclidean loss层， 并调整网络结构。

